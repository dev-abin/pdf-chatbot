Step 2 — Generate fingerprint (Deterministic + GenAI hybrid) — detailed plan

Goal: produce a single, normalized fingerprint JSON for a target XML node by combining deterministic structural extraction with GenAI-assigned semantic labels. Deterministic code must be the source of truth for structural facts; GenAI only enriches semantics (labels, sensitivity, intent, likely canonical types) and resolves ambiguous naming. The output must be verifiable, reproducible, and auditable.


---

Inputs & output

Inputs

One or more normalized XML node samples (N ≥ 3 recommended; N = 1 allowed with lower confidence).

Extraction configuration (which node XPath to target; rules for order-significance).

Deterministic rules: canonical namespace map, redact rules (timestamps/GUIDs).


Primary output

fingerprint_json with fields: node_path, node_name, namespace, structural_summary, identity_model, document_references, value_patterns, semantics, constraints, formatting, namespace_usage, metadata.



---

Pipeline stages (concrete)

1. Pre-normalization (deterministic)

Parse fragment into DOM / ElementTree.

Canonicalize namespaces to their URI form; strip irrelevant prefixes.

Sort child nodes when order is not significant (configurable per node type).

Redact or replace volatile values with placeholders (e.g., __TIMESTAMP__, __GUID__).

Normalize whitespace and attribute ordering.



2. Structural extraction (deterministic)

Compute canonical absolute node_path (XPath using canonical namespace URIs).

Enumerate direct child elements and their local-names with namespaces.

For each child element:

Count occurrences across samples → map to cardinality one|zero-or-one|many.

Determine if order is significant (based on config or heuristics: repeated sibling names usually unordered).

Capture attributes present on node and children, with occurrence frequency.


Detect text-only vs mixed-content children.

Capture attribute-vs-element for candidate identity fields (e.g., elements/attributes named id, ref, No, Number, or patterns like .*id|.*Id|id.*).

Detect ID/IDREF usage via XML ID type (if schema/DTD available) or heuristics: attributes referenced elsewhere as @ref, href="#...", @id uniqueness within document sample.



3. Reference & identity model detection (deterministic)

For identified candidate ID attributes/elements:

Test uniqueness within sample set (is value unique among siblings?).

Check cross-sample references: do other nodes reference this value (href, @ref, IDREF)? If yes → external_reference.


Decide identity_location:

inline if unique ID exists within the node and is used as canonical identity.

external_reference if node contains only a ref to an external identity.

both if node has own ID and references elsewhere.

none if no stable identifier found.




4. Value-pattern inference (deterministic + heuristics)

For common field names (passport, ssn, birthdate, email, phone) run deterministic regex candidate matches over observed values to propose patterns.

Infer date formats by parsing (ISO vs dd/mm/yyyy etc.). Store canonical format list.

For other textual fields, compute basic stats: average length, char-class distribution (alpha/numeric), sample examples.



5. Aggregation across samples (deterministic rules)

For each structural property (child set, attributes), compute majority rule across samples:

If ≥ 80% samples contain the child → required else optional.


For multiplicity, use thresholds:

count=0 across samples → zero

count always 1 → one

sometimes 0 sometimes 1 → zero-or-one

sometimes >1 → many


Produce source_sample_count, field_presence_rates in metadata.



6. GenAI role — semantic enrichment (controlled)

Input to GenAI: canonicalized fragment(s) plus deterministic facts extracted above (child list, candidate identity fields, sample values). Ask GenAI to produce only semantics keys: inferred conceptual labels (e.g., passport_number → passport), role_concepts (e.g., traveler, infant), sensitive_fields, recommended value_patterns if deterministic heuristics were inconclusive, and short human-friendly field_descriptions.

GenAI must return structured JSON only (no prose). The pipeline validates GenAI outputs against deterministic facts; any GenAI-assigned identity assertions are downgraded if they contradict deterministic uniqueness/reference checks.



7. Post-processing & validation (deterministic)

Merge deterministic and GenAI outputs:

Structural facts and identity_model come from deterministic extractor.

Add semantics from GenAI, but mark each semantic item with confidence and source:"genai".


Run consistency checks:

If GenAI marks Passport/Number as sensitive but deterministic shows no such element → flag mismatch.

Ensure cardinality fields are normalized and consistent.


Compute signature_hash (SHA256) over a sorted, canonical serialization of fingerprint fields that are deterministic (exclude GenAI confidence fields if you want hash stable across reruns).

Compute a flattened feature_vector for indexing:

Example vector fields: num_child_elements, num_attrs, has_inline_id (0/1), has_external_ref (0/1), num_sensitive_fields, num_namespaces, plus one-hot or hashed tokens for frequently occurring child names.




8. Finalize metadata

confidence = weighted function of:

sample_count (more samples → higher),

%agreement across samples for cardinality/identity,

GenAI semantic confidence (if provided).


Add provenance: sample file ids, timestamps, extractor version, model id & prompt hash.





---

GenAI prompt template (strict, output-only JSON)

Use few-shot plus explicit schema. Below is a compact template and two-shot examples you can plug into your GenAI extractor.

Prompt skeleton

You are given: (1) canonicalized XML fragment(s) (with namespaces normalized), and (2) deterministic facts extracted: {child_elements, attributes, identity_candidates, value_examples}. 

Task: Return only JSON with fields:
{
  "semantics": {
    "field_labels": [{"path":"Passenger/Passport/Number","label":"passport_number","confidence":0.0}],
    "role_concepts": ["traveler","lead-passenger"],
    "sensitive_fields": ["Passenger/Passport/Number"],
    "field_descriptions": [{"path":"Passenger/Name","desc":"Passenger full name"}]
  }
}

Be conservative. If unsure, return lower confidence values. Do NOT invent elements that are not present. Output valid JSON only.

Two-shot examples (very short)

1. Example input: a passenger fragment with <Passenger PassengerID="P001"><Name><Given>John</Given></Name><Passport><Number>A1234567</Number></Passport></Passenger> and deterministic: identity_candidates: ["@PassengerID"], value_examples: {"Passport/Number":["A1234567"]}
Expected GenAI output:



{
  "semantics": {
    "field_labels":[{"path":"Passenger/@PassengerID","label":"passenger_id","confidence":0.95}],
    "role_concepts":["traveler"],
    "sensitive_fields":["Passenger/Passport/Number"],
    "field_descriptions":[{"path":"Passenger/Passport/Number","desc":"International passport number"}]
  }
}

2. Example input: a PassengerRef node with <PassengerRef ref="#P001"/> and deterministic: identity_candidates: ["@ref"], referenced_id_in_samples:true
Expected GenAI output:



{
  "semantics": {
    "field_labels":[{"path":"PassengerRef/@ref","label":"passenger_reference","confidence":0.9}],
    "role_concepts":["traveler"],
    "sensitive_fields":[],
    "field_descriptions":[{"path":"PassengerRef/@ref","desc":"Reference to inline passenger id"}]
  }
}


---

Deterministic extractor — concise pseudocode

def extract_structural_facts(xml_fragment):
    dom = parse(xml_fragment)
    canon_ns = canonicalize_namespaces(dom)
    children = list_direct_children(dom)
    attributes = list_attributes(dom)
    sample_values = collect_sample_values(dom)
    identity_candidates = find_candidates(children, attributes, name_patterns=["id","ref","no","number"])
    id_uniqueness = test_uniqueness(identity_candidates, across_samples=True)
    id_references = detect_href_idrefs(dom, document_context)
    cardinalities = compute_cardinalities(children, across_samples)
    value_patterns = infer_value_patterns(sample_values)
    return {
       "node_path": canonical_xpath(dom),
       "children": children,
       "attributes": attributes,
       "identity_candidates": identity_candidates,
       "id_uniqueness": id_uniqueness,
       "id_references": id_references,
       "cardinalities": cardinalities,
       "value_patterns": value_patterns
    }


---

Confidence scoring (how to derive)

structural_confidence = min(1.0, 0.5 + 0.05 * sample_count) capped by agreement: multiply by average presence rate of required fields.

identity_confidence = 1.0 if deterministic uniqueness + cross-reference detected; else lower.

semantic_confidence = genai_reported_confidence * 0.8 (downweight LLM).

Final confidence = weighted(structural_confidence=0.6, identity_confidence=0.3, semantic_confidence=0.1).


Store the per-field confidence to enable explainable diff thresholds.


---

Validation & safeguards

Reject or flag any GenAI suggestion that contradicts deterministic uniqueness or reference checks (mark as genai_suggestion and set accepted:false).

Keep raw sample fragments and a copy of the deterministic facts in the DB for audit.

Log extractor version and model prompt hash; regenerate fingerprints deterministically if rules change to enable re-validation.



---

Edge cases & notes

Single-sample input: still produce fingerprint but set confidence low; mark fields inferred from single sample as sample_limited:true.

Mixed namespace styles: canonicalize to URIs; record original prefixes in metadata for human debugging.

Schema available: if XSD/WSDL provided, use it to validate and augment structural facts deterministically; treat XSD as authoritative for cardinalities.

Large fragments: apply fragment sampling (pick representative nodes) to avoid processing excessive data.



---


Short answer — yes

Exactly. Your plan is sound: for each relevant XML section produce a normalized fingerprint JSON, persist those fingerprints in a database, and when a new XML section arrives produce its fingerprint and compute similarity against stored fingerprints to produce a match score and a small explainable diff.

Recommended end-to-end flow

1. Extract relevant fragment(s) from the incoming XML (e.g., /Booking/Passengers).


2. Normalize the fragment set (canonicalize namespaces, sort unordered children, redact non-deterministic values like timestamps/GUIDs).


3. Generate fingerprint JSON for the normalized fragment (GenAI can infer semantics like “identity is inline” or “passport field”), plus deterministic structural facts.


4. Store fingerprint + provenance in DB (fingerprint JSON, normalized sample fragments, sample-count, confidence, computed signature/hash, and an embedding/vector).


5. Index fingerprints for fast nearest-neighbor (vector index and hash index).


6. On new fragment: repeat steps 1–3 to get new fingerprint.


7. Compare new fingerprint to database entries with a hybrid comparator (vector similarity, structural/tree distance, identity-model checks).


8. Return similarity score(s) with a breakdown of contributing differences and a suggested classification (match / variant / different).



Minimal DB record schema (recommended fields)

id (pk)

pattern_name (optional human label)

fingerprint_json (raw JSON)

signature_hash (SHA256 of normalized fingerprint)

vector (dense vector for semantic fields or flattened structural features)

sample_fragments (array or separate table)

source_count (int)

created_at, updated_at

confidence (float)

metadata (freeform — namespace URIs, version tags)


Comparison design (hybrid, explainable)

Combine three signals:

Semantic/structural vector similarity (cosine on embedding / engineered vector) — captures high-level similarity.

Tree-edit distance (TED) or normalized subtree structural distance — captures structural changes (added/removed children, attribute↔element changes).

Key/identity-model check (binary/heavy-weight penalty) — e.g., inline ID vs external reference mismatch → large negative impact.


Aggregate into weighted score, example:

score = 0.45 * cosine_sim
      + 0.35 * (1 - normalized_TED)
      + 0.20 * key_match_score

Interpretation guideline:

>= 0.85 → same pattern (confident)

0.65–0.85 → variant (minor schema evolution or optional fields)

< 0.65 → different pattern


Tune weights by validation on labelled pairs.

How to make fingerprints comparable

Use deterministic normalization before fingerprinting.

Canonicalize element/attribute names to local-name + canonical namespace URI.

Represent multiplicities as one|zero-or-one|many.

Represent identity model explicitly (inline|external|both|none).

Produce a compact flattened feature vector for indexing (counts, flags, one-hots for identity_location, number of sensitive fields, number of namespaces, etc.).

Also compute a semantic embedding (small vector) from selected textual fields (names of elements, inferred semantic labels) for vector search.


Indexing & performance options

Small scale: use PostgreSQL JSONB + GIN for metadata + ANN library (pgvector).

Medium/large scale: vector DB (FAISS/HNSW/Chroma/Weaviate) for vector search + secondary hash map (signature_hash) for exact matches.

Pre-filter by cheap checks (namespace match, identity_model match) before running TED.


Explainability & diffs

Always surface:

Which fields differ (identity_location changed, attribute→element, new child added).

Structural delta (child added/removed) and sample value mismatch rates for regexed fields (e.g., passport fails pattern).

Confidence and provenance (which samples produced the stored fingerprint).


Practical safeguards

Use deterministic parser for structural extraction; reserve GenAI for semantic labeling (field intent, sensitive flag inference).

Store original sample fragments to re-evaluate fingerprints when you update generation rules.

Version fingerprints so you can evolve patterns and track drift.


Next deliverables I can produce (pick any)

A compact GenAI prompt + two-shot examples to produce the fingerprint JSON.

Fingerprint → flattened feature vector spec and a JSON→vector mapping example.

Comparator pseudocode (hybrid scoring + thresholds) and an explainable diff output format.

SQL/DDL and small schema for PostgreSQL + pgvector (lightweight).

Example walkthrough: compare two pasted passenger fragments and produce fingerprints + score.


Which of those should I produce now?
