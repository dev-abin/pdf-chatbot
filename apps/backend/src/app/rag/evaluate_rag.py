import json
import uuid
from datetime import UTC, datetime

from langchain_ollama import OllamaLLM

from ..core.logging_config import logger, rag_logger
from ..core.settings import (
    OLLAMA_API_URL,
    PREF_EMBEDDING_MODEL,
    PREF_MODEL,
)


# ----------------- RAG interaction logging -----------------
def log_interaction(query, contexts, llm_response, ground_truth=None):
    """
    Log user interaction details for a Retrieval-Augmented Generation query.
    """
    interaction = {
        "interaction_id": str(uuid.uuid4()),
        "timestamp": datetime.now(UTC).isoformat(),
        "query": query,
        "contexts": contexts,
        "llm_response": llm_response,
        "ground_truth": ground_truth,
    }
    rag_logger.info(interaction)
    return True


# Load logged data
def load_logged_interactions(filepath):
    """Load logged interaction records from a JSON log file.

    This function reads a log file containing JSON-formatted interaction records generated
    by the Retrieval-Augmented Generation (RAG) system. Each line in the file is
    expected to be a JSON object, nested  with a 'message' field containing the
    interaction data. The function extracts relevant fields (query, response, contexts, and
    ground truth) and returns them as a list of structured dictionaries.

    Args:
        filepath (str): The path to the log file containing JSON-formatted interaction records.

    Returns:
        list: A list of dictionaries, each containing:
            - user_input (str): The user's query or question.
            - response (str): The response generated by the language model.
            - retrieved_contexts (list): A list of retrieved context strings used to generate the response.
            - reference (str or None): The ground truth or expected response, if available.
    """
    # Initialize an empty list to store the parsed records
    records = []

    try:
        # Open the log file in read mode
        with open(filepath) as f:
            for line in f:
                # Parse the line as a JSON object
                raw = json.loads(line)

                # Check if the JSON object has a nested 'message' field
                if "message" in raw:
                    # Parse the nested 'message' field as another JSON object
                    data = json.loads(raw["message"])

                    # Create a structured record with relevant fields
                    record = {
                        "user_input": data["query"],
                        "response": data["llm_response"],
                        "retrieved_contexts": data["contexts"],
                        "reference": data["ground_truth"],
                    }

                    # Append the record to the list
                    records.append(record)

    except Exception:
        # Catch any errors, log them, and return an empty list
        logger.exception("load_logged_interactions failed")
        return []

    # Return the list of parsed records
    return records


def evaluate_rag_response():
    """Evaluate the performance of a RAG system using logged interaction data.

    This function loads the most recent interaction record from a JSONL log file and evaluates
    the Retrieval-Augmented Generation (RAG) system's performance using the `ragas` library.
    It assesses the response using metrics such as Faithfulness, Context Precision, and Response
    Relevancy, leveraging an Ollama LLM and HuggingFace embeddings. The evaluation results are
    logged, and the function returns True to indicate successful execution.

    Args:
        None: This function does not take any parameters

    Returns:
        bool: Returns True if the evaluation was successful, False if an error occurs
    """
    try:
        import os

        from langchain_huggingface import HuggingFaceEmbeddings
        from ragas import EvaluationDataset, evaluate
        from ragas.embeddings import LangchainEmbeddingsWrapper
        from ragas.llms import LangchainLLMWrapper
        from ragas.metrics import (
            Faithfulness,
            LLMContextPrecisionWithoutReference,
            ResponseRelevancy,
        )
        from ragas.run_config import RunConfig

        # Get absolute path to the directory of the current script
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))

        # Construct full path to logs directory
        log_file_path = os.path.join(BASE_DIR, "logs", "rag_interactions.jsonl")

        # get the logged records
        records = load_logged_interactions(log_file_path)

        # Select only the most recent record for evaluation
        records = [records[-1]]

        # Create an EvaluationDataset from the selected record
        dataset = EvaluationDataset.from_list(records)

        # Initialize the evaluator LLM using Ollama with predefined model and API URL
        evaluator_llm = LangchainLLMWrapper(
            OllamaLLM(model=PREF_MODEL, base_url=OLLAMA_API_URL)
        )

        # spcify the embedding fn
        embedding_fn = HuggingFaceEmbeddings(model_name=PREF_EMBEDDING_MODEL)

        # Wrap the embeddings for compatibility with ragas
        embedding = LangchainEmbeddingsWrapper(embedding_fn)

        # Define the metrics for evaluation
        metrics = [
            LLMContextPrecisionWithoutReference(),  # Measures precision of retrieved contexts
            Faithfulness(),  # Assesses factual accuracy of the response
            ResponseRelevancy(),  # Evaluates relevance of the response to the query
        ]

        # Configure evaluation to run with 1 concurrent metric call and a 300-second timeout
        run_config = RunConfig(timeout=300, max_workers=1)

        # Perform the evaluation using the dataset, metrics, LLM, and embeddings
        results = evaluate(
            dataset=dataset,
            metrics=metrics,
            llm=evaluator_llm,
            embeddings=embedding,
            run_config=run_config,
        )

        logger.info(results)

        return True

    except Exception:
        # Log any unexpected errors during evaluation and return False
        logger.exception("evaluate_rag_response failed")
        return False


if __name__ == "__main__":
    evaluate_rag_response()
